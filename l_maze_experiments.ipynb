{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/Projects/lm-pcg/evaluate.py:122: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  def main(args: Config):\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from conf.config import Config\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datasets import GameDataset, SokobanLMDataset, LMazeLMDataset\n",
    "from evaluate import evaluate\n",
    "from utils import get_run_name, save_train_state, load_train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    model = \"gpt2\"\n",
    "    game = \"l_maze\"\n",
    "    data_source = \"l_maze\"\n",
    "    chunk_size = 128\n",
    "    learning_rate = 1e-4\n",
    "    exp_name = \"\"\n",
    "    seed = 1\n",
    "\n",
    "    gen_len = 128\n",
    "    gen_temp = 1.0\n",
    "    gen_beams = 5\n",
    "    gen_top_k = 50\n",
    "    gen_top_p = 1.0\n",
    "    gen_typical_p = 1.0\n",
    "    num_eval_samples = 50\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load checkpoint from ./logs/l_maze/gpt2/l_maze/chunk_size-128_lr-0.0001/seed-1...\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the checkpoint\n",
    "run_name = get_run_name(args)\n",
    "output_dir = f\"./logs/{run_name}\"\n",
    "\n",
    "model, _, global_step = load_train_state(output_dir)\n",
    "\n",
    "model_mapping = {\"gpt2\": \"gpt2\",\n",
    "                     \"codeparrot\": \"lvwerra/codeparrot\",\n",
    "                     \"java-gpt2\": \"microsoft/CodeGPT-small-java-adaptedGPT2\",\n",
    "                     \"incoder-1B\": \"facebook/incoder-1B\",\n",
    "                     \"incoder-6B\": \"facebook/incoder-6B\"}\n",
    "\n",
    "# Instantiate the tokenizer based on the model's name\n",
    "model_name = model_mapping[args.model]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"PAD\",\n",
    "                                \"bos_token\": \"START\"})\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokens from cache at ./caches/gpt2_l_mazes_train_all_token_ids.npy...\n"
     ]
    }
   ],
   "source": [
    "# Create the LMaze dataset\n",
    "dataset = LMazeLMDataset(tokenizer,\n",
    "                         args.model,\n",
    "                         chunk_size=args.chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION PARAMETERS:\n",
      "\tLength: 128\n",
      "\tTemperature: 1.0\n",
      "\tTop-k: 50\n",
      "\tTop-p: 1.0\n",
      "\tTypical-p: 1.0\n",
      "\tBeams: 5\n",
      "\n",
      "Proportion accurate: 1.0\n",
      "Proportion playable: 1.0\n",
      "Proportion novel: 1.0\n"
     ]
    }
   ],
   "source": [
    "prop_accurate, prop_novel, prop_playable = evaluate(model, device, tokenizer, dataset, args, verbose=False, render_dir=None)\n",
    "\n",
    "print(\"GENERATION PARAMETERS:\")\n",
    "print(f\"\\tLength: {args.gen_len}\")\n",
    "print(f\"\\tTemperature: {args.gen_temp}\")\n",
    "print(f\"\\tTop-k: {args.gen_top_k}\")\n",
    "print(f\"\\tTop-p: {args.gen_top_p}\")\n",
    "print(f\"\\tTypical-p: {args.gen_typical_p}\")\n",
    "print(f\"\\tBeams: {args.gen_beams}\")\n",
    "\n",
    "print(f\"\\nProportion accurate: {prop_accurate}\")\n",
    "print(f\"Proportion playable: {prop_playable}\")\n",
    "print(f\"Proportion novel: {prop_novel}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('lm-pcg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24775f06f7acbd1479eb0bc92e07ea1bc459991f8de84028f9bafb207e7d5502"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
